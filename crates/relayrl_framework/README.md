# RelayRL Framework

**Core Library for Deep Multi-Agent Reinforcement Learning**

---
**Version:** 0.5.0-alpha

**Status:** Under active development, expect breaking changes!

**Tested Platform Support:** macOS (Silicon), Linux (Ubuntu), Windows 10 (x86_64)

## Overview

With v0.5.0 being a complete rewrite of v0.4.5's client implementation, the `relayrl_framework` crate now provides a **multi-actor native** client runtime for deep reinforcement learning experiments. The training server (and new inference server) are under development and remain unavailable in this update.

As of now, the only way to perform inference is to provide your own `LibTorch` or `ONNX` model formatted to the framework's standardized `ModelModule` interface. Upon implementation of the training server and algorithms, the client will be able to acquire a `ModelModule` from the training server's algorithm runtime just like in v0.4.5.

All feature flags other than `client` are (more) unstable - if not entirely unimplemented - and unsuitable for RL experiment usage. Use at your own risk!

**Key Features:**

- **Multi-actor native architecture** with concurrent actor execution
- Local Arrow file sink for **offline trajectory data collection** and training
- **Scalable** router-based message dispatching for actor runtimes
- **Ergonomic builder pattern** API for agent construction

**Current Limitations:**

- **Data Collection:** Only local Arrow file sink is available
- **Transport Layer:** Network transport (ZMQ) is under active development
- **Database Layer:** PostgreSQL/SQLite support is under active development

**Major Changes:**

- **Architecture Redesign:** Monolithic design of v0.4.5 abstracted into a decoupled layered architecture, enhancing modularity, maintainability, and testability.
- **Rust-First Design Philosophy:** Complete removal of PyO3 and its Python code dependencies from framework; all core components written entirely in Rust.
- **Backend Independence:** Replacement of direct `Tch` crate dependency with `Burn`, enabling generic Tensor interfacing with the framework (currently supports Burn's `Tch` and `NdArray` Tensor backends, as well as `LibTorch` and `ONNX` model inference).
- **Improved Error Handling:** Near complete removal of panics and replacement with proper error handling (retries, branches, etc.) and upstream propagation.
- **Tonic/gRPC Removal:** All Tonic-related code has been removed with focus being cast on building a strong `ZMQ` transport implementation.
- **Type System:** Moved to a separate crate (`relayrl_types`).
- **RL Algorithms:** Moved to a separate crate (`relayrl_algorithms`), which remains unimplemented for now.
- **Python Bindings:** Moved to a separate crate (`relayrl_python`), which remains unimplemented for now.

## Quick Start

```rust
use relayrl_framework::prelude::network::{RelayRLAgent, AgentBuilder, RelayRLAgentActors};
use relayrl_framework::prelude::types::{ModelModule, DeviceType};
use burn_ndarray::NdArray;
use burn_tensor::{Tensor, Float};
use std::path::PathBuf;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // 1. Build and Start
    const OBS_RANK: usize = 2;
    const ACT_RANK: usize = 2;

    let model_path = PathBuf::from("dummy_model");
    
    // The builder initializes with default local inference settings
    let (mut agent, params) = AgentBuilder::<NdArray, OBS_RANK, ACT_RANK, Float, Float>::builder()
        .actor_count(4)
        .default_model(ModelModule::<NdArray>::load_from_path(model_path))
        .build().await?;

    // Start the agent using the parameters generated by the builder
    agent.start(
        params.actor_count, 
        params.router_scale, 
        params.default_device, 
        params.default_model, 
        params.config_path
    ).await?;

    // 2. Interact (using Burn Tensors)
    let reward: f32 = 1.0;
    let obs = Tensor::<NdArray, OBS_RANK, Float>::zeros([1, 4], &Default::default());
    
    // get_actor_ids returns a Result
    let ids = agent.get_actor_ids()?; 
    
    let acts = agent.request_action(ids.clone(), obs, None, reward).await?;
    let versions = agent.get_model_version(ids.clone()).await?;

    // 3. Actor Runtime Management
    agent.new_actor(DeviceType::Default, None).await?;
    
    let new_actor_count: u32 = 10;
    agent.new_actors(new_actor_count, DeviceType::Default, None).await?;
    
    let ids = agent.get_actor_ids()?;
    if ids.len() >= 2 {
        agent.set_actor_id(ids[0], uuid::Uuid::new_v4()).await?;
        agent.remove_actor(ids[1]).await?;
    }

    // 4. Agent Management and Shutdown
    let last_reward: Option<f32> = Some(3.0);
    let ids = agent.get_actor_ids()?;
    agent.flag_last_action(ids.clone(), last_reward).await?;
    
    agent.scale_throughput(2).await?;
    
    agent.shutdown().await?;
    
    Ok(())
}
```

## Usage Instructions

[View this guide for agent usage :)](CLIENT_GUIDE)

## Roadmap

- ### **v0.5.0:**
  - Client `ZMQ` transport interface completion
  - Client `PostgreSQL` and `SQLite` database interface completion
  - Comprehensive Client testing and benchmarking on common gym environments
  - Short Client stabilization period to enable focused server-side development

- ### **v0.6.0:**
  - Training Server implementation with support for Online/Offline training workflows
  - `relayrl_algorithms` crate integration to enable deep RL algorithmic training and Client `ModelModule` acquisition
  - Comprehensive Training Server testing and benchmarking
  - Comprehensive Client-Training Server network testing and benchmarking on common gym environments
  - Momentary Training Server stabilization

- ### **v0.7.0:**
  - Inference Server implementation to provide client with remote inference capabilities
  - Inference Server and Training Server communication for updating Inference Server's inference model(s)
  - Comprehensive Inference Server testing and benchmarking
  - Comprehensive Client-Inference Server-Training Server network testing and benchmarking on common gym environments

- ### **Beyond this crate:**
  - `relayrl_algorithms` crate creation and publication for training workflows
  - `relayrl_types` updates to minimize serialization overhead and to reduce tensor copy towards zero-copy (preferably)
  - `relayrl_cli` for ease-of-use and language agnostic execution via a deployable gRPC pipeline for external CLI process interfacing

## Contributing

Contributions are welcomed! Please open issues or pull requests for bug reports, feature requests, or improvements. I'll be glad to work with you!

## License

[Apache License 2.0](../../LICENSE)